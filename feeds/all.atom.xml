<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Note on IML</title><link href="http://amosz.github.io/Note-on-Introduction-to-Machine-Learning/" rel="alternate"></link><link href="http://amosz.github.io/Note-on-Introduction-to-Machine-Learning/feeds/all.atom.xml" rel="self"></link><id>http://amosz.github.io/Note-on-Introduction-to-Machine-Learning/</id><updated>2013-11-11T00:00:00Z</updated><entry><title>Chapter1-2</title><link href="http://amosz.github.io/Note-on-Introduction-to-Machine-Learning/chapter1-2.html" rel="alternate"></link><updated>2013-11-11T00:00:00Z</updated><author><name>Amos Zhu and Enoche Zhou</name></author><id>tag:amosz.github.io/Note-on-Introduction-to-Machine-Learning,2013-11-11:chapter1-2.html</id><summary type="html">&lt;hr /&gt;
&lt;h1&gt;Chapter1 : Introduction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; consist of two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;learn from  experience(history data)&lt;/li&gt;
&lt;li&gt;adapt to environments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In past, I usually confuse &lt;strong&gt;Machine Learning&lt;/strong&gt; and &lt;strong&gt;Data Mining&lt;/strong&gt;. &lt;em&gt;Data Mining&lt;/em&gt; is an application of machine learning methods to large databases. But machine learning is also a part of artificial intelligence. Something is called to be "Intelligent" means it has the abiliy to &lt;strong&gt;learn and adapter to changes&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I don't think machine learning is just the execution of a computer program to optimize the parameters of a model. It should also tell us how to select or create the model. &lt;strong&gt;Learning&lt;/strong&gt; should not merely optimize the parameters of an known formular(model,hypothesis).&lt;/p&gt;
&lt;h2&gt;Problem:&lt;/h2&gt;
&lt;h3&gt;1. What kind of learning algorithm (Supervised/Unsupervised/Reinforement) do you think is most suitable for pattern formation (e-Mouse is used) task in our lab.? If we change the task to navigation issue?&lt;/h3&gt;
&lt;p&gt;A:I think we should understand what is supervised/unsupervised/reinforemen learning formally at first.(To be added).
Reinforement is the best choice I think. We don't have any history data to make prediction and the emouse won't get together automatically in the form of letter A. Instead, each emouse should take a sequence steps to form a pattern. It is what reinforement learning does. &lt;/p&gt;
&lt;hr /&gt;
&lt;h1&gt;Chapter2: Supervised Learning&lt;/h1&gt;
&lt;h2&gt;2.1 An example: Label family car automatically.&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Training Data&lt;/strong&gt;: a set of examples cars.Each of them has a label.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Input Representation:&lt;/strong&gt; Here,we separate a family car from other cars are the &lt;strong&gt;price&lt;/strong&gt; and &lt;strong&gt;engine power&lt;/strong&gt;. But do we have some general method or idea to represent input? It is very important to know which attributes(or dimension) of data play main role in machine learning.&lt;/p&gt;
&lt;p&gt;Some symbols:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{H}$: &lt;em&gt;hypothesis class&lt;/em&gt;. It is a model with parameters unknow.&lt;/li&gt;
&lt;li&gt;$h$: hypothesis. $h \in \mathcal{H}$. It is a model with known parameters.&lt;/li&gt;
&lt;li&gt;$X$: tranning set.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Empirical error&lt;/strong&gt; : the proportion of training instances where predictions of $h$ do not match the required values $r$ given in $X$
$$
    E(h|\mathcal{X}) = \sum_{t=1}^{N}(h(x_t) \neq r_t)
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We know that there maybe infinite $h \in \mathcal{H}$ whose empirical error is zero(or maybe none, see below). How to select the best one? --The hypothesis with max &lt;strong&gt;margin&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So here is our general steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;find the &lt;em&gt;most specific hypothesis&lt;/em&gt; S that is the &lt;strong&gt;tightest boundary&lt;/strong&gt; that include all the positive example and none of the negative example.&lt;/li&gt;
&lt;li&gt;find the &lt;em&gt;most general hypothesis&lt;/em&gt; G that is the &lt;strong&gt;largest boundary&lt;/strong&gt; that include all the positive example and none of the negative example. (Do we have some more mathematical definition of specific hypothesis and general hypothesis??)&lt;/li&gt;
&lt;li&gt;find a way to compute &lt;strong&gt;margin&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;choose the hypothesis $h$ from  hypothesis class $\mathcal{H}$ with &lt;strong&gt;max margin&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We will know that &lt;strong&gt;SVM(support vector machine)&lt;/strong&gt; will produce a hypossis with largest margin. What is the relationship between them? &lt;/p&gt;
&lt;p&gt;In the end of this example. It tell us that we assume there exist $h \in \mathcal{H}$ with $E(h|\mathcal{X})$ is 0.But how we can always choose a hypothesis $\mathcal{X}$ that include this $h$? Just as mentioned above, machine learning should tell us how to create/select/produce a hypothsis class(model with unknown parameters), not merely optimize parameter with a known model.&lt;/p&gt;
&lt;h2&gt;2.2 Vapnik-Chervonenkis(VC) Dimension.&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Shatter&lt;/strong&gt; : A classification model $f$ with some parameter vector $\vec{\theta}$ is said to "shatter" a set of data point , if, for all assignments of labels to thoses points, there exists a $\theta$ such that model $f$ make no error when evaluating that set of data points.&lt;/p&gt;
&lt;p&gt;$\mathcal{H}$ : is the hypothesis class.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The maximum number of pointers that can be &lt;strong&gt;shattered&lt;/strong&gt; by $\mathcal{H}$(there exist $h \in \mathcal{H}$ can separate $\mathcal{C}$ perfectly(with no empirical error)) is called the &lt;em&gt;Vapnik-Chervonenkis(VC) dimension&lt;/em&gt; of $\mathcal{H}$, is denoted as$VC(\mathcal{H})$ and measure the &lt;em&gt;capacity of $\mathcal{H}$&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why Vapnik-Chervonenkis(VC) dimension is useful????&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reference on Vapnik-Chervonenkis(VC) dimension: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory"&gt;http://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/VC_dimension"&gt;http://en.wikipedia.org/wiki/VC_dimension&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2.3 Probably Approximately Correct (PAC) Learning&lt;/h2&gt;
&lt;h3&gt;Definition&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Probably Approximately Correct (PAC) Learning&lt;/strong&gt; provide us a method to caculate how many examples(training data) do we need if we use &lt;strong&gt;tightest hypothesis&lt;/strong&gt;. &lt;/p&gt;
&lt;p&gt;The confidence probability $1-\delta$ is the factor of our certainty on the conculsion : error probability at most $\epsilon$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In &lt;em&gt;Probably Approximately Correct (PAC) learning&lt;/em&gt;, given a class, $\mathcal{C}$, and examples drawn from some unknown but fixed probability distribution, $\mathcal{p(x)}$, we want to find the number of examples, $N$, such that with probability at least $1 - \delta$, the hypothesis $h$ has error at most $\epsilon$, for arbitrary $\delta \le 1/2$ and $\epsilon &amp;gt; 0$&lt;/p&gt;
&lt;p&gt;$$P\{\mathcal{C} \Delta h \le \epsilon \} \ge 1 - \delta$$&lt;/p&gt;
&lt;p&gt;where $\mathcal{C} \Delta h$ is the region of difference between $\mathcal{C}$ and hypothesis $h$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Explanation:&lt;/h3&gt;
&lt;p&gt;We know that $h$ is the tightest hypothsis which is just a boundary around the known(or selected) positive points. 
And all points(examples) are drawn from an unknown but fixed probability distribution. 
We can't make sure that $\mathcal{C} \Delta h$ is less that $\epsilon$.Instead, we give it a lower limit probability : $1 - \delta$. 
This is why the name of this learning is :Probably Approximately Correct&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approximately means : we use a (tightest) hypothesis to approximate the absolutely correct class: C&lt;/li&gt;
&lt;li&gt;Probably means : we can't make sure of that. We can only give a lower limit probability : $1 - \delta$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Derivation:&lt;/h3&gt;
&lt;p&gt;$\mathcal{C} \Delta h &amp;lt; \epsilon$ can be derived from each strip is $ &amp;lt; \epsilon/4$. &lt;/p&gt;
&lt;p&gt;Then we know that $h$ is totally determined by points(examples). &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Sub-conclusion:&lt;/strong&gt;
Making degree of strip is T, the event:&lt;/p&gt;
&lt;p&gt;$T \geq \epsilon/4$ == no point(example) is in the region of $\epsilon/4$ == all points fallen in the region out of $\epsilon/4$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prove:&lt;/strong&gt;
Assuming that degree of strip $T \geq \epsilon/4$ and there is a point fallen into this region, the tightest hypothessis will contain this point. Then T should smaller than $\epsilon/4$. It conflict with our assume. Approved&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then we can get 
$$
    N \geq \frac{4}{\epsilon}ln{\frac{4}{\delta}}
$$&lt;/p&gt;
&lt;h3&gt;Problems:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; On page $P69$ of Chapter 2, the author gives an example about how to calculate the number of samples needed to satisfy the miss value $\delta$. The author uses upper bound $\epsilon / 4$ to measure the error, note in here $\epsilon / 4$ is the upper bound for one strip. Then the actually error, noted as $\alpha$, then $\alpha \le \epsilon$. Why not calculated by $(1 - \alpha)^N$ instead of $(1 - \epsilon / 4)^N$?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; I think they could be fine. But method is this book are more strict. Because if $\alpha &amp;gt; 0, x &amp;gt; 0 $, $\alpha(1 - \frac{x}{\alpha})^n &amp;gt; (1-x)^n$&lt;/p&gt;
&lt;p&gt;So if $4(1-\epsilon/4)^n \leq \delta$ then $(1-\epsilon)^n \leq \delta$&lt;/p&gt;
&lt;h3&gt;Conclusion:&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;I think the pac gives a us way to quantify the value of proximity between the underlying distribution and our hypothesis under a number of samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't think so. PAC tell us if we want to adopt tightest hypothesiss with confidence probability at least $1 − \delta$, a given point will be misclassified with error probability at most $\epsilon$ &lt;/p&gt;
&lt;h2&gt;2.7 Model selection and Generalization&lt;/h2&gt;
&lt;p&gt;Why there are $2^2^d$ possible Boolean functions of d inputs?&lt;/p&gt;</summary></entry></feed>